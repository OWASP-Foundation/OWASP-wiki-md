![468x60-banner-2010.gif](468x60-banner-2010.gif
"468x60-banner-2010.gif")

[Registration](https://guest.cvent.com/EVENTS/Register/IdentityConfirmation.aspx?e=d52c6f5f-d568-4e16-b8e0-b5e2bf87ab3a)
|
[Hotel](https://resweb.passkey.com/Resweb.do?mode=welcome_gi_new&groupID=2766908)
| [Walter E. Washington Convention
Center](http://www.dcconvention.com/)
\== The presentation ==

![Owasp_logo_normal.jpg](Owasp_logo_normal.jpg
"Owasp_logo_normal.jpg")While the growth of web applications has been
extraordinary, there are no structured methodologies for evaluating
static code scanners. We introduce a structured methodology to evaluate
static code scanners with core areas of emphasis: compatibility,
vulnerability detection, reporting functionality, and usability. Our
methodology evaluates static code scanners so development teams can
compare multiple tools with the same processes and against the same test
bed. We leverage WebGoat as a well-accepted vulnerable application so
all vulnerabilities are explicitly known and false positives and false
negatives can be included as part of our evaluation methodology. Our
methodology can be applied to any static code scanner, and when followed
correctly will score every tool with same criteria and scoring
processes. We completed a case study to evaluate two static source code
scanners to illustrate our methodology and to ensure the appropriate
criteria and scoring mechanisms are present.

## The speaker

Speaker bio will be posted shortly.

[Category:AppSec_DC_2010_Turbo_Talks](Category:AppSec_DC_2010_Turbo_Talks "wikilink")
[Category:OWASP_Conference_Presentations](Category:OWASP_Conference_Presentations "wikilink")